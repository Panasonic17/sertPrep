1. в папці на hdfs лежить csv без хідера з стркутурою a a1 a2 a3 a4 a5 a6 ...     завдання взяти всі поля де а1>=10  , посортувати по a3 по спаданню  збереги колонки a4 a5 a6 на хдфс з таб деліметром 
2. зчитати з джейсона, погрупувати і найти середнє
3. з хайву зчитати і шось там зробити, не помню, але фігня якась, фільтр і сорт, шось таке
4. порахувати ерор логи через акумулятор
5. зчитати текст файл і тоже якісь там фільтрації і т.д. Прикол був в тмоу шо там получався датафрейм де одне поле була дата в форматі "2012-10-02", а треба було взяти записи лиш з роком 2012
6. зчитати з двох хайв таблиць і заджойнити, потім пофільтрувати там по чомусь
7. спарк сабміт в клієнт моді

{"phone_number":"PHONE","rec_date":"REC_DATE","channel":"CHANNEL","plan":"PLAN","amount":"AMOUNT"} -third table
{"phone_number":"PHONE_NUM","plan":"PLAN","rec_date":"REC_DATE","status":"STAUS","balance":"BALANCE","imei":"IMEI","region":"REGION"} -customer details 

1 sc.textFile("hdfs:/test").filter(s=> !s.contains("code")).map(r=>r.split('\t')).filter(a=>a(3)=="64").sortBy(a=>a(0),true,1).saveAsTextFile("hdfs:/ta/blablabla")
2  sqlContext.read.json("hdfs:/task2/customer_details - Copy.json1").groupBy("region").agg(avg("imei").as("gd")).coalesce(1).write.json("hdfs:/te/jsontext1")

1  sc.textFile("hdfs:/task2/table2.csv").map(row=>row.split(",")).filter(arr=>arr(4).toInt>100).sortBy(arr=>arr(0).trim.toLong,false,1).map(arr=>Array(arr(0),arr(4),arr(3))).map(MKSTRING)saveAsTextFile("hdfs:/task2/t11")
2  sqlContext.read.json("hdfs:/task2/thirdTeble - Copy.json1").withColumn("chanel",$"channel".cast("Int")).groupBy("amount").agg(avg("chanel").as("sawa")).saveAsTable("task22")
3  sqlContext.sql("select * from sad").withColumn("rrp",$"rrp".cast("Double")).groupBy("brand").agg(max("category"),avg("stock")).show
4  val acc=sc.accumulator(0)
   sqlContext.sql("select * from sad").rdd.map(r=>r.getString(3)).foreach(b=>{if (b.equals("Nike")) acc+=1})
5  val myUDF =udf {a:String=> a+"sawa"}
   sqlContext.sql("select * from sad").withColumn("pid",myUDF($"pid")).show

   sqlContext.udf.register("myudf",(s:String) => {s + "sawa"})
   sqlContext.sql("select myudf(pid) from sad ").show

6  sqlContext.sql("select * from sad ").join(sqlContext.sql("select * from sad")).show

create database sert;
sc.textFile("/te/items.csv").map(r=>r.split("\\|")).map(arr=>(arr(0),arr(1),arr(2),arr(3),arr(4),arr(5),arr(6),arr(7),arr(8),arr(9))).toDF("pid","size","color","brand","rrp","mainCategory","category","subCategory","stock","releaseDate").saveAsTable("sert.items")
sc.textFile("/te/train.csv").map(r=>r.split("\\|")).map(arr=>(arr(0),arr(1),arr(2),arr(3))).toDF("date","pid","size","units").saveAsTable("sert.train")


tuples start from 1 but array from 0 
in spark sort == orderby 
sqlContext.sql("select * from xademo.recharge_details" ).withColumn("all",concat($"amount",lit(","),$"phone_number")).show(23,false)

just regular 
sqlContext.sql("select * from xademo.recharge_details" ).withColumn("all",concat($"amount",$"phone_number")).show(23,false)
sqlContext.sql("select * from xademo.recharge_details" ).rdd.map(r=>Array(r.getString(0),r.getString(1),r.getString(2),r.getString(3),r.getString(4)).mkString(",")).saveAsTextFile("hdfs:/test/csv1")

1 sc.textFile("file:/sert/items.csv").filter(r=> !r.contains("pid")).map(d=>d.split("\\|")).map(arr=>(arr(0),arr(1),arr(2),arr(3),arr(4),arr(5),arr(6),arr(7),arr(8),arr(9))).filter(t=>t._1.toInt%2==0).sortBy(t=>t._7.toInt,true,1).map(t=>Array(t.2,t.4,t.5).mkString("sawa"))).saveAsTextFile("hdfs:/te/task1234")

2 sqlContext.read.json("file:/sert/train.json").groupBy("color").count.coalesce(1).write.json("hdfs:/te/zxcvb")
2 sqlContext.read.json("file:/sert/train.json").withColumn("pid",$"pid".cast("Int")).groupBy("color").avg("pid").show

3 sqlContext.sql("Select * from sert.items where size ='L' or size='S'").sort("subCategory").saveAsTable("sad")


4  sc.accumulator(0)
   sc.textFile("file:/sert/train.json").foreach(r=>{if (r.contains("XL")) { res4+=1 }  })

5   val myupper: String => String = _.toUpperCase
    val myudf=udf(myupper)
    sqlContext.sql("select * from sert.train").withColumn("nre",myudf($"size")).show

    val t1 = udf { name: String => name.split("\\|")(0) }
    df.withColumn("change", t1($"json"))

    val parceData : String =>Int = _.split("-")(0).toInt
    val parceDataUDF=udf(parceData)
    sqlContext.sql("select * from sert.train where date!='date'").withColumn("year",parceDataUDF($"date")).show

    val test =udf {s:String => s+"SAWA CAN CREATE UDF "}
    sqlContext.sql("select * from sert.train where date!='date'").withColumn("year",test($"date")).show

6   t2.join(t1,"movieid").show 
    t2.join(t1,$"imdbid"==="userid").show
    t2.join(t1,$"imdbid"==="userid","joinType").show




